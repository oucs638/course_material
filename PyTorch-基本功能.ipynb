{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch-基本功能\n",
    "\n",
    "## 教學目標\n",
    "\n",
    "這份教學的目標是介紹 PyTorch，撰寫深度學習模型的函式庫。\n",
    "\n",
    "## 適用對象\n",
    "\n",
    "已經有基本的機器學習知識，且擁有 python、`numpy`、`matplotlib` 基礎的學生。\n",
    "\n",
    "若沒有先學過 python，請參考 [python-入門語法](./python-入門語法.ipynb) 教學。\n",
    "\n",
    "若沒有先學過 `numpy`，請參考 [numpy-基本功能](./numpy-基本功能.ipynb) 教學。\n",
    "\n",
    "若沒有先學過 `matplotlib`，請參考 [matplotlib-資料視覺化](./matplotlib-資料視覺化.ipynb) 教學。\n",
    "\n",
    "## 執行時間\n",
    "\n",
    "本教學全部執行時間約為 4.376506090164185 秒。\n",
    "\n",
    "|測試環境|名稱|\n",
    "|-|-|\n",
    "|主機板|X570 AORUS ELITE|\n",
    "|處理器|AMD Ryzen 7 3700X 8-Core Processor|\n",
    "|記憶體|Kingston KHX3200C16D4/16GX|\n",
    "|硬碟|Seagate ST1000DM003-1ER1|\n",
    "|顯示卡|GeForce RTX 2080|\n",
    "|作業系統|Ubuntu 18.04 LTS|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 大綱\n",
    "\n",
    "- [簡介](#簡介)\n",
    "- [安裝](#安裝)\n",
    "- [張量宣告](#張量宣告)\n",
    "- [張量取值](#張量取值)\n",
    "- [張量運算](#張量運算)\n",
    "- [創造張量](#創造張量)\n",
    "- [高維張量運算](#高維張量運算)\n",
    "- [維度運算](#維度運算)\n",
    "- [使用 GPU 運算](#使用-GPU-運算)\n",
    "- [深度學習](#深度學習)\n",
    "- [練習](#練習)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 簡介\n",
    "\n",
    "根據 [PyTorch 官方網站](https://pytorch.org/)（v1.4）：\n",
    "\n",
    "> PyTorch is an open source machine learning framework that accelerates the path from research prototyping to production deployment.\n",
    "> \n",
    "> PyTorch 是一個開源的機器學習框架，能夠幫助加速從研究原型到商業應用的轉換過程。\n",
    "\n",
    "![PyTorch usage statistics](https://thegradient.pub/content/images/2019/10/ratio_medium-1.png)\n",
    "\n",
    "根據[統計](https://thegradient.pub/state-of-ml-frameworks-2019-pytorch-dominates-research-tensorflow-dominates-industry/)，PyTorch 在各大機器學習會議使用率逐年上升，使用者選擇 PyTorch 的原因為：\n",
    "\n",
    "- 簡單（Simplicity）\n",
    "    - 使用 `python` 作為介面\n",
    "    - 操作方法與 `numpy` 相似\n",
    "- 好用的介面（Great API）\n",
    "    - 沒有過多的抽象化\n",
    "- 效能（Performance）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 安裝\n",
    "\n",
    "請參考 [PyTorch 官方網站](https://pytorch.org/get-started/locally/#start-locally)，並選擇適合的環境選項與安裝方法。\n",
    "\n",
    "本教學使用 `pip` 安裝 `torch`，選項如下：\n",
    "\n",
    "|選項|描述|選擇|\n",
    "|-|-|-|\n",
    "|PyTorch Build|請選**穩定版**避免未知錯誤|`Stable(1.4)`|\n",
    "|Your OS|依照**作業系統**來選擇|`Linux`|\n",
    "|Package|安裝 **PyTorch** 使用的方法|`Pip`|\n",
    "|Language|當前執行 **Python** 版本|`Python 3.6`|\n",
    "|CUDA|電腦上是否有 **GPU** 且支援 **CUDA 架構**|`10.1`|\n",
    "\n",
    "得到以下安裝指令：\n",
    "\n",
    "```sh\n",
    "pip install torch torchvision\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 匯入 PyTorch 套件\n",
    "# 在 python 中的介面名稱為 torch\n",
    "import torch\n",
    "\n",
    "# 匯入 numpy 與 matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print((\n",
    "    'PyTorch version {}\\n' +\n",
    "    'GPU-enabled installation? {}'\n",
    ").format(\n",
    "    torch.__version__,        # 確認 torch 的版本\n",
    "    torch.cuda.is_available() # 確認是否有 GPU 裝置\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 張量宣告\n",
    "\n",
    "在 `torch` 中陣列稱為張量（Tensor），創造張量的語法為 `torch.tensor([value1, value2, ...])`。\n",
    "\n",
    "- 每個 `torch.Tensor` 都有不同的**數值型態屬性** `torch.Tensor.dtype`\n",
    "    - 必須透過 `torch.Tensor.dtype` 取得，無法透過 `type()` 取得\n",
    "- 可以指定型態\n",
    "    - 透過參數 `dtype` 指定型態\n",
    "    - 透過 `torch.LongTensor` 創造整數，預設為 `torch.int64`\n",
    "    - 透過 `torch.FloatTensor` 創造浮點數，預設為 `torch.float32`\n",
    "\n",
    "|`torch` 型態|`numpy` 型態|C 型態|範圍|\n",
    "|-|-|-|-|\n",
    "|`torch.int8`|`numpy.int8`|`int_8`|-128~127|\n",
    "|`torch.int16`|`numpy.int16`|`int_16`|-32768~32767|\n",
    "|`torch.int32`|`numpy.int32`|`int_32`|-2147483648~2147483647|\n",
    "|`torch.int64`|`numpy.int64`|`int_64`|-9223372036854775808~9223372036854775807|\n",
    "|`torch.float32`|`numpy.float32`|`float`||\n",
    "|`torch.float64`|`numpy.float64`|`double`||\n",
    "\n",
    "- 每個 `torch.Tensor` 都有**維度屬性** `torch.Size`\n",
    "    - 呼叫 `torch.Tensor.size()` 來取得維度屬性\n",
    "    - `torch.Tensor.size` 本質是 `tuple`\n",
    "    - 張量維度愈高，`len(torch.Tensor.size)` 數字愈大\n",
    "- 可以使用 `torch.Tensor.reshape` 或 `torch.Tensor.view` 進行維度變更\n",
    "    - 變更後的維度必須要與變更前的維度乘積相同\n",
    "    - 變更後的內容為 **shallow copy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 張量宣告\n",
    "\n",
    "t1 = torch.tensor([1, 2, 3])                           # 宣告 Tensor 變數\n",
    "print(t1)                                              # 輸出 Tensor\n",
    "print(type(t1) == torch.Tensor)                        # 輸出 True\n",
    "print(t1.dtype)                                        # 輸出 torch.int64\n",
    "print()\n",
    "\n",
    "t2 = torch.tensor([1., 2., 3.])                        # 宣告 Tensor 變數\n",
    "print(t2)                                              # 輸出 Tensor\n",
    "print(type(t2) == torch.Tensor)                        # 輸出 True\n",
    "print(t2.dtype)                                        # 輸出 torch.float32\n",
    "print()\n",
    "\n",
    "# 各種 dtype\n",
    "print(torch.tensor([1, 2], dtype=torch.int8).dtype)    # 輸出 torch.int8\n",
    "print(torch.tensor([1, 2], dtype=torch.int16).dtype)   # 輸出 torch.int16\n",
    "print(torch.tensor([1, 2], dtype=torch.int32).dtype)   # 輸出 torch.int32\n",
    "print(torch.tensor([1, 2], dtype=torch.int64).dtype)   # 輸出 torch.int64\n",
    "print(torch.tensor([1, 2], dtype=torch.float32).dtype) # 輸出 torch.float32\n",
    "print(torch.tensor([1, 2], dtype=torch.float64).dtype) # 輸出 torch.float64\n",
    "print()\n",
    "\n",
    "t3 = torch.LongTensor([1, 2, 3])                       # 宣告 LongTensor 變數\n",
    "print(t3.dtype)                                        # 輸出 torch.int64\n",
    "\n",
    "t4 = torch.FloatTensor([1, 2, 3])                      # 宣告 FloatTensor 變數\n",
    "print(t4.dtype)                                        # 輸出 torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# size 屬性\n",
    "\n",
    "t5 = torch.tensor([               # 宣告 Tensor 變數\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "    [7, 8, 9],\n",
    "    [10, 11, 12],\n",
    "])\n",
    "\n",
    "print(t5)                         # 輸出 Tensor\n",
    "print(t5.size())                  # 輸出 t5.size (4, 3)\n",
    "print()\n",
    "\n",
    "print(t5.reshape(3, 4))           # 重新更改 t5.size\n",
    "print(t5.reshape(3, 4).size())    # 輸出更改後的維度 (3, 4)\n",
    "print()\n",
    "print(t5.view(3, 4))              # 重新更改 t5.size\n",
    "print(t5.view(3, 4).size())       # 輸出更改後的維度 (3, 4)\n",
    "print()\n",
    "\n",
    "print(t5.reshape(2, 6))           # 重新更改 t5.size\n",
    "print(t5.reshape(2, 6).size())    # 輸出更改後的維度 (2, 6)\n",
    "print()\n",
    "print(t5.view(2, 6))              # 重新更改 t5.size\n",
    "print(t5.view(2, 6).size())       # 輸出更改後的維度 (2, 6)\n",
    "print()\n",
    "\n",
    "print(t5.reshape(2, 3, 2))        # 重新更改 t5.size\n",
    "print(t5.reshape(2, 3, 2).size()) # 輸出更改後的維度 (2, 3, 2)\n",
    "print()\n",
    "print(t5.view(2, 3, 2))           # 重新更改 t5.size\n",
    "print(t5.view(2, 3, 2).size())    # 輸出更改後的維度 (2, 3, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 張量取值\n",
    "\n",
    "與 `numpy` 語法概念相似。\n",
    "\n",
    "- 使用 `torch.Tensor[位置]` 來取得 `torch.Tensor` 中指定位置的值\n",
    "    - 若為**多個維度**的張量，則使用 `tuple` 來取得指定位置的值\n",
    "    - 若位置為**負數**，則等同於反向取得指定位置的值\n",
    "    - 取出的值會以 `torch.Tensor.dtype` 的形式保留\n",
    "- 使用 `torch.Tensor[起始位置:結束位置]` 來取得 `torch.Tensor` 中的部分**連續**值\n",
    "    - **包含起始位置**的值\n",
    "    - **不包含結束位置**的值\n",
    "    - 取出的值會以 `torch.Tensor` 的形式保留\n",
    "- 使用 `torch.Tensor[iterable]`（例如 `list`, `tuple` 等）來取得**多個** `torch.Tensor` 中的值\n",
    "    - 取出的值會以 `torch.Tensor` 的形式保留\n",
    "- 使用判斷式來取得 `torch.Tensor` 中的部份資料\n",
    "    - 經由判斷式所得結果也為 `torch.Tensor`\n",
    "    - 判斷式所得結果之 `torch.Tensor.dtype` 為**布林值** `bool`（`True` 或 `False`）\n",
    "    - 取出的值會以 `torch.Tensor` 的形式保留"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 張量取值\n",
    "\n",
    "t6 = torch.tensor([ # 宣告 Tensor 變數\n",
    "    [0, 1, 2],\n",
    "    [3, 4, 5],\n",
    "    [6, 7, 8],\n",
    "    [9, 10, 11],\n",
    "])\n",
    "\n",
    "print(t6[0])        # 輸出張量 t6 中的第 0 個位置的值 [0, 1, 2]\n",
    "print(t6[1])        # 輸出張量 t6 中的第 1 個位置的值 [3, 4, 5]\n",
    "print(t6[2])        # 輸出張量 t6 中的第 1 個位置的值 [6, 7, 8]\n",
    "print(t6[-2])       # 輸出張量 t6 中的第 -2 個位置的值 [6, 7, 8]\n",
    "print(t6[-1])       # 輸出張量 t6 中的第 -1 個位置的值 [9, 10, 11]\n",
    "print()\n",
    "\n",
    "print(t6[0, 0])     # 輸出張量 t6 中的第 [0, 0] 個位置的值 0\n",
    "print(t6[0, 1])     # 輸出張量 t6 中的第 [0, 1] 個位置的值 1\n",
    "print(t6[1, 1])     # 輸出張量 t6 中的第 [1, 1] 個位置的值 4\n",
    "print(t6[1, 2])     # 輸出張量 t6 中的第 [1, 2] 個位置的值 5\n",
    "print(t6[-1, -1])   # 輸出張量 t6 中的第 [-1, -1] 個位置的值 11\n",
    "print(t6[-1, -2])   # 輸出張量 t6 中的第 [-1, -2] 個位置的值 10\n",
    "print(t6[-2, -1])   # 輸出張量 t6 中的第 [-2, -1] 個位置的值 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取連續值\n",
    "\n",
    "t7 = torch.tensor([ # 宣告 Tensor 變數\n",
    "    0, 10, 20, 30, 40, \n",
    "    50, 60, 70, 80, 90\n",
    "])\n",
    "\n",
    "print(t7[0:3])      # 輸出張量 t7 位置 0, 1, 2 但是不含位置 3 的值 [0, 10, 20]\n",
    "print(t7[7:])       # 輸出張量 t7 位置 7, 8, 9 的值 [70, 80, 90]\n",
    "print(t7[:2])       # 輸出張量 t7 位置 0, 1 但是不含位置 2 的值 [0, 10]\n",
    "print(t7[:])        # 輸出張量 t7 所有位置的值 [0, 10, 20, 30, 40, 50, 60, 70, 80, 90]\n",
    "print()\n",
    "\n",
    "t8 = torch.tensor([ # 宣告 Tensor 變數\n",
    "    [0, 1, 2],\n",
    "    [3, 4, 5],\n",
    "    [6, 7, 8],\n",
    "    [9, 10, 11],\n",
    "])\n",
    "\n",
    "print(t8[0:2])      # 輸出張量 t8 位置 0, 1, 但是不含位置 2 的值 [[0, 1, 2], [3, 4, 5]]\n",
    "print()\n",
    "print(t8[1:])       # 輸出張量 t8 位置 1, 2, 3 的值 [[3, 4, 5], [6, 7, 8], [9, 10, 11]]\n",
    "print()\n",
    "print(t8[:1])       # 輸出張量 t8 位置 0 但是不含位置 1 的值 [[0, 1, 2]]\n",
    "print()\n",
    "print(t8[:])        # 輸出張量 t8 位置 0 但是不含位置 1 的值 [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 10, 11]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 iterable 取得多個值\n",
    "\n",
    "t9 = torch.tensor([        # 宣告 Tensor 變數\n",
    "    0, 10, 20, 30, 40, \n",
    "    50, 60, 70, 80, 90\n",
    "])\n",
    "\n",
    "print(t9[[0, 2, 4, 6, 8]]) # 輸出張量 t9 中偶數位置的值 [0, 20, 40, 60, 80]\n",
    "print()\n",
    "print(t9[[1, 3, 5, 7, 9]]) # 輸出張量 t9 中奇數位置的值 [10, 30, 50, 70, 90]\n",
    "print()\n",
    "\n",
    "t10 = torch.tensor([       # 宣告 Tensor 變數\n",
    "    [1, 2, 3, 4],\n",
    "    [5, 6, 7, 8],\n",
    "    [9, 10, 11, 12]\n",
    "])\n",
    "\n",
    "print(t10[[0, 1]])         # 輸出張量 t10[0] 與 t10[1] 的值 [[1, 2, 3, 4] [5, 6, 7, 8]]\n",
    "print()\n",
    "print(t10[[0, 1], [2, 3]]) # 輸出張量 t10[0, 2] 與 t10[1, 3] 的值 [3, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 判斷式取值\n",
    "\n",
    "t11 = torch.tensor([      # 宣告 Tensor 變數\n",
    "    0, 10, 20, 30, 40, \n",
    "    50, 60, 70, 80, 90\n",
    "])\n",
    "\n",
    "print(t11 > 50)           # 輸出每個值是否大於 50 的 `torch.Tensor`\n",
    "print((t11 > 50).dtype)   # 輸出 torch.bool\n",
    "print(t11[t11 > 50])      # 輸出大於 50 的值 [60, 70, 80, 90]\n",
    "print(t11[t11 % 20 == 0]) # 輸出除以 20 餘數為 0 的值 [0, 20, 40, 60, 80]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 張量運算\n",
    "\n",
    "### 純量運算（Scalar Operation）\n",
    "\n",
    "對張量內所有數值與單一純量（Scalar）進行相同計算。\n",
    "\n",
    "|符號|意義|\n",
    "|-|-|\n",
    "|`torch.Tensor + scalar`|張量中的每個數值加上 `scalar`|\n",
    "|`torch.Tensor - scalar`|張量中的每個數值減去 `scalar`|\n",
    "|`torch.Tensor * scalar`|張量中的每個數值乘上 `scalar`|\n",
    "|`torch.Tensor / scalar`|張量中的每個數值除以 `scalar`|\n",
    "|`torch.Tensor // scalar`|張量中的每個數值除以 `scalar` 所得之商|\n",
    "|`torch.Tensor % scalar`|張量中的每個數值除以 `scalar` 所得之餘數|\n",
    "|`torch.Tensor ** scalar`|張量中的每個數值取 `scalar` 次方|\n",
    "\n",
    "### 個別數值運算（Element-wised Operation）\n",
    "\n",
    "若兩個張量想要進行運算，則兩個張量的**維度必須相同**（即兩張量之 `torch.size()` 相同）。\n",
    "\n",
    "|符號|意義|\n",
    "|-|-|\n",
    "|`A + B`|張量 `A` 中的每個數值加上張量 `B` 中相同位置的數值|\n",
    "|`A - B`|張量 `A` 中的每個數值減去張量 `B` 中相同位置的數值|\n",
    "|`A * B`|張量 `A` 中的每個數值乘上張量 `B` 中相同位置的數值|\n",
    "|`A / B`|張量 `A` 中的每個數值除以張量 `B` 中相同位置的數值|\n",
    "|`A // B`|張量 `A` 中的每個數值除以張量 `B` 中相同位置的數值所得之商|\n",
    "|`A % B`|張量 `A` 中的每個數值除以張量 `B` 中相同位置的數值所得之餘數|\n",
    "|`A ** B`|張量 `A` 中的每個數值取張量 `B` 中相同位置的數值之次方|\n",
    "\n",
    "### 個別數值函數運算（Element-wised Functional Operation）\n",
    "\n",
    "若想對張量中的**所有數值**進行**相同函數運算**，必須透過 `torch` 提供的介面進行。\n",
    "\n",
    "|函數|意義|\n",
    "|-|-|\n",
    "|`torch.sin`|張量中的每個數值 $x$ 計算 $\\sin(x)$|\n",
    "|`torch.cos`|張量中的每個數值 $x$ 計算 $\\cos(x)$|\n",
    "|`torch.tan`|張量中的每個數值 $x$ 計算 $\\tan(x)$|\n",
    "|`torch.exp`|張量中的每個數值 $x$ 計算 $e^{x}$|\n",
    "|`torch.log`|張量中的每個數值 $x$ 計算 $\\log x$\n",
    "|`torch.ceil`|張量中的每個數值 $x$ 計算 $\\left\\lceil x \\right\\rceil$\n",
    "|`torch.floor`|張量中的每個數值 $x$ 計算 $\\left\\lfloor x \\right\\rfloor$\n",
    "\n",
    "### 張量自動擴充（Broadcasting）\n",
    "\n",
    "若張量 `A` 的維度為 `(a1, a2, ..., an)`（即 `A.size() == (a1, a2, ..., an)`），則張量 `B` 在滿足以下其中一種條件時即可與張量 `A` 進行運算：\n",
    "\n",
    "- 張量 `B` 與張量 `A` 維度相同（即 `B.size() == (a1, a2, ..., an)`）\n",
    "- 張量 `B` 為純量（即 `B.size() == (1,)`）\n",
    "- 張量 `B` 的維度為 `(b1, b2, ..., bn)`，若 `ai != bi`，則 `ai == 1` 或 `bi == 1`\n",
    "    - 從**最後**一個維度開始比較\n",
    "    - 如果有任何一個維度無法滿足前述需求，則會得到 `ValueError`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 純量運算\n",
    "\n",
    "t12 = torch.tensor([ # 宣告 Tensor 變數\n",
    "    [0, 10, 20],\n",
    "    [30, 40, 50],\n",
    "    [60, 70, 80],\n",
    "    [90, 100, 110],\n",
    "])\n",
    "\n",
    "print(t12)           # 輸出張量 t12\n",
    "print()\n",
    "print(t12 + 5)       # 對張量 t12 所有數值加 5\n",
    "print()\n",
    "print(t12 - 4)       # 對張量 t12 所有數值減 4\n",
    "print()\n",
    "print(t12 * 3)       # 對張量 t12 所有數值乘 3\n",
    "print()\n",
    "print(t12 / 10)      # 對張量 t12 所有數值除以 10\n",
    "print()\n",
    "print(t12 // 10)     # 對張量 t12 所有數值除以 10 所得整數部份\n",
    "print()\n",
    "print(t12 % 7)       # 對張量 t12 所有數值除以 7 得到餘數\n",
    "print()\n",
    "print(t12 ** 2)      # 對張量 t12 所有數值取 2 次方"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 個別數值運算\n",
    "\n",
    "t13 = torch.tensor([ # 宣告 Tensor 變數\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6]\n",
    "])\n",
    "\n",
    "t14 = torch.tensor([ # 宣告 Tensor 變數\n",
    "    [6, 5, 4],\n",
    "    [3, 2, 1]\n",
    "])\n",
    "\n",
    "print(t13 + t14)     # 張量相加\n",
    "print()\n",
    "print(t13 - t14)     # 張量相減\n",
    "print()\n",
    "print(t13 * t14)     # 張量相乘\n",
    "print()\n",
    "print(t13 / t14)     # 張量相除\n",
    "print()\n",
    "print(t13 // t14)    # 張量相除取商\n",
    "print()\n",
    "print(t13 % t14)     # 張量相除取餘數\n",
    "print()\n",
    "print(t13 ** t14)    # 張量 A 取張量 B 次方"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 個別數值函數運算\n",
    "\n",
    "t15 = torch.tensor([               # 宣告 Tensor 變數\n",
    "    [0,     np.pi / 4,     np.pi / 2,     np.pi / 4 * 3],\n",
    "    [np.pi, np.pi / 4 * 5, np.pi / 2 * 3, np.pi / 4 * 7]\n",
    "])\n",
    "\n",
    "print(torch.sin(t15))              # 張量所有數值計算 sine\n",
    "print()\n",
    "print(torch.cos(t15))              # 張量所有數值計算 cosine\n",
    "print()\n",
    "print(torch.tan(t15))              # 張量所有數值計算 tangent\n",
    "print()\n",
    "\n",
    "t16 = torch.tensor([               # 宣告 Tensor 變數\n",
    "    [1., 2., 3.],\n",
    "    [4., 5., 6.]\n",
    "])\n",
    "\n",
    "print(torch.exp(t16))              # 張量所有數值取指數\n",
    "print()\n",
    "print(torch.log(t16))              # 張量所有數值取對數\n",
    "print()\n",
    "print(torch.ceil(torch.log(t16)))  # 張量所有數值取對數後無條件進位\n",
    "print()\n",
    "print(torch.floor(torch.log(t16))) # 張量所有數值取對數後無條件捨去"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 張量自動擴充\n",
    "\n",
    "t17 = torch.tensor([ # 宣告 Tensor 變數\n",
    "    [\n",
    "        [1, 2],\n",
    "        [3, 4],\n",
    "        [5, 6],\n",
    "    ],\n",
    "    [\n",
    "        [7, 8],\n",
    "        [9 ,10],\n",
    "        [11, 12]\n",
    "    ]\n",
    "])\n",
    "\n",
    "t18 = torch.tensor([ # 宣告 Tensor 變數\n",
    "    [\n",
    "        [1],\n",
    "        [1],\n",
    "        [1]\n",
    "    ],\n",
    "    [\n",
    "        [2],\n",
    "        [2],\n",
    "        [2]\n",
    "    ],\n",
    "])\n",
    "\n",
    "print(t17.size())    # 輸出張量 t17 維度\n",
    "print(t18.size())    # 輸出張量 t18 維度\n",
    "print()\n",
    "print(t17 + t17)     # 張量 t17 與張量 t17 維度相同，所以可以直接運算\n",
    "print()\n",
    "print(t17 + t18)     # 張量 t17 與張量 t18 可以擴充成相同維度，所以可以運算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 創造張量\n",
    "\n",
    "### 賦值（Assignment）\n",
    "\n",
    "使用 `=` 賦與指定位置數值。可以使用 `iterable` 一次指定多個位置。\n",
    "\n",
    "|符號|意義|\n",
    "|-|-|\n",
    "|`=`|賦值|\n",
    "|`+=`|進行加法後賦值|\n",
    "|`-=`|進行減法後賦值|\n",
    "|`*=`|進行乘法後賦值|\n",
    "\n",
    "### 隨機（Random）\n",
    "\n",
    "創造出新的張量，所有數值皆為**隨機決定**，必須**事先指定張量維度**。\n",
    "\n",
    "|函數|意義|用途|備註|\n",
    "|-|-|-|-|\n",
    "|`torch.empty`|創造隨機未初始化張量|已確認維度，尚未確認數值|無法控制隨機|\n",
    "|`torch.rand`|創造隨機浮點數張量|需要隨機浮點數時|透過均勻分佈決定亂數，範圍介於 0 到 1之間|\n",
    "|`torch.randn`|創造隨機浮點數張量，並符合常態分佈|需要符合常態分佈的隨機浮點數時|透過常態分佈決定亂數，$\\mu = 0$ 且 $\\sigma = 1$|\n",
    "|`torch.randint`|創造隨機整數張量|需要隨機整數時|透過均勻分佈決定亂數，可以控制隨機範圍|\n",
    "\n",
    "### 指定數值（Filled In）\n",
    "\n",
    "**快速創造**擁有特定數值的張量，必須**事先指定張量維度**。\n",
    "\n",
    "|函數|意義|用途|\n",
    "|-|-|-|\n",
    "|`torch.zeros`|創造指定維度大小的張量，所有數值初始化為 0|快速初始化|\n",
    "|`torch.zeros_like`|複製指定張量的維度，創造出新的張量，所有數值初始化為 0|複製張量並初始化|\n",
    "|`torch.ones`|創造指定維度大小的張量，所有數值初始化為 1|快速初始化|\n",
    "|`torch.ones_like`|複製指定張量的維度，創造出新的張量，所有數值初始化為 1|複製張量並初始化|\n",
    "|`torch.full`|創造指定維度大小的張量，所有數值初始化為指定數值|快速初始化|\n",
    "|`torch.full_like`|複製指定張量的維度，創造出新的張量，所有數值初始化為指定數值|複製張量並初始化|\n",
    "|`torch.eye`|創造單位矩陣|矩陣微分|\n",
    "|`torch.arange`|列舉數字|等同於 `list(range(value))`|\n",
    "\n",
    "### 從 numpy 轉換\n",
    "\n",
    "可以使用 `torch.tensor()` 將 `numpy.ndarray` 轉換成 `torch.Tensor`；\n",
    "使用 `torch.numpy()` 將 `torch.Tensor` 轉換成 `numpy.ndarray`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 賦值\n",
    "\n",
    "t19 = torch.tensor([     # 宣告 Tensor 變數\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "    [7, 8, 9],\n",
    "    [10, 11, 12]\n",
    "])\n",
    "print(t19)\n",
    "print()\n",
    "\n",
    "t19[0] = 1995            # 將張量 t19 位置 0 的所有數值改成 1995\n",
    "print(t19)\n",
    "print()\n",
    "\n",
    "t19[0, 1] = 10           # 將張量 t19 位置 [0, 1] 的所有數值改成 10\n",
    "print(t19)\n",
    "print()\n",
    "\n",
    "t19[[2, 0], [1, 2]] = 12 # 將張量 t19 位置 [2, 1] 與 [0, 2] 的所有數值改成 12\n",
    "print(t19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t20 = torch.tensor([      # 宣告 Tensor 變數\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "    [7, 8, 9],\n",
    "    [10, 11, 12]\n",
    "])\n",
    "print(t20)\n",
    "print()\n",
    "\n",
    "t20[0] += 1995            # 將張量 t20 位置 0 的所有數值加上 1995\n",
    "print(t20)\n",
    "print()\n",
    "\n",
    "t20[0, 1] -= 10           # 將張量 t20 位置 [0, 1] 的所有數值減掉 10\n",
    "print(t20)\n",
    "print()\n",
    "\n",
    "t20[[2, 0], [1, 2]] *= 12 # 將張量 t20 位置 [2, 1] 與 [0, 2] 的所有數值乘上 12\n",
    "print(t20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 隨機\n",
    "\n",
    "print(torch.empty((2, 3)))               # 隨機創造維度為 (2, 3) 的張量\n",
    "print()                                  # 數值為無法控制範圍的浮點數\n",
    "\n",
    "print(torch.rand(2, 3))                  # 隨機創造維度為 (2, 3) 的張量\n",
    "print()                                  # 數值為介於 0 到 1 之間的浮點數\n",
    "\n",
    "print(torch.rand(2, 3) * 10)             # 隨機創造維度為 (2, 3) 的張量\n",
    "print()                                  # 數值為介於 0 到 10 之間的浮點數\n",
    "\n",
    "print(torch.rand(2, 3) * 10 - 5)         # 隨機創造維度為 (2, 3) 的張量\n",
    "print()                                  # 數值為介於 -5 到 5 之間的浮點數\n",
    "\n",
    "print(torch.randn(2, 3))                 # 隨機創造維度為 (2, 3) 的張量\n",
    "print()                                  # 分佈為平均值為 0 標準差為 1 的常態分佈\n",
    "\n",
    "print(torch.randint(-5, 5, size=(2, 3))) # 隨機創造維度為 (2, 3) 的張量\n",
    "                                         # 數值為介於 -5 到 5 之間的浮點數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定數值\n",
    "\n",
    "print(torch.zeros((2, 3)))      # 創造維度為 (2, 3) 的張量，並初始化為 0\n",
    "print()\n",
    "\n",
    "t21 = torch.tensor([            # 宣告 Tensor 變數\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "])\n",
    "print(torch.zeros_like(t21))    # 複製張量 t21 的維度，創造出新的張量，並初始化為 0\n",
    "print()\n",
    "\n",
    "print(torch.ones((3, 4)))       # 創造維度為 (3, 4) 的張量，並初始化為 1\n",
    "print()\n",
    "\n",
    "t22 = torch.tensor([            # 宣告 Tensor 變數\n",
    "    [1, 2, 3, 4],\n",
    "    [5, 6, 7, 8],\n",
    "    [9, 10, 11, 12]\n",
    "])\n",
    "print(torch.ones_like(t22))     # 複製張量 t22 的維度，創造出新的張量，並初始化為 1\n",
    "print()\n",
    "\n",
    "print(torch.full((5, 6), 420))  # 創造維度為 (5, 6) 的張量，並初始化為 420\n",
    "print()\n",
    "\n",
    "t23 = torch.tensor([            # 宣告 Tensor 變數\n",
    "    [1, 2, 3, 4, 5, 6],\n",
    "    [7, 8, 9, 10, 11, 12],\n",
    "    [13, 14, 15, 16, 17, 18],\n",
    "    [19, 20, 21, 22, 23, 24],\n",
    "    [25, 26, 27, 28, 29, 30]\n",
    "])\n",
    "print(torch.full_like(t23, 69)) # 複製張量 t23 的維度，創造出新的張量，並初始化為 69"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.eye(3))           # 創造 3x3 單位矩陣\n",
    "print()\n",
    "\n",
    "print(torch.arange(10))       # 從 0 列舉至 10，但不包含 10\n",
    "print()\n",
    "\n",
    "print(torch.arange(6, 9))     # 從 6 列舉至 9，但不包含 9\n",
    "print()\n",
    "\n",
    "print(torch.arange(4, 20, 7)) # 從 4 遞增至 20，但不包含 20，每次遞增 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 從 numpy 轉換\n",
    "\n",
    "arr1 = np.array([1., 2., 3.]) # 宣告 ndarray 變數\n",
    "t24 = torch.tensor(arr1)      # 將 numpy.ndarray 轉換為 torch.Tensor\n",
    "arr2 = t24.numpy()            # 將 torch.Tensor 轉換為 numpy.ndarray\n",
    "\n",
    "print((\n",
    "    'original numpy.ndarray: {}, dtype: {}\\n' + \n",
    "    'converted torch.Tensor: {}, dtype: {}\\n' +\n",
    "    'converted numpy.ndarray: {}, dtype: {}'\n",
    ").format(\n",
    "    arr1, arr1.dtype,\n",
    "    t24, t24.dtype,\n",
    "    arr2, arr2.dtype\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 高維張量運算\n",
    "\n",
    "矩陣等同於是維度為 2 的張量。\n",
    "而高維度的張量運算等同於**固定大部分的維度**，只使用**其中的兩個維度進行計算**。\n",
    "\n",
    "### 張量乘法（Tensor Multiplication）\n",
    "\n",
    "令 $A$ 與 $B$ 為兩張量，$A.\\text{size}() = (a_1, a_2, ..., a_{n - 1}, a_n)$, $B.\\text{size}() = (b_1, b_2, ..., b_{n - 1}, b_n)$。定義 $A \\times B$ 如下：\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "a_i &= b_i \\forall i \\in \\{1, \\dots, n - 2\\} \\\\\n",
    "a_n &= b_{n - 1} \\\\\n",
    "(A \\times B).\\text{size}() &= (d_1, d_2, \\dots, d_{n - 2}, a_{n - 1}, b_n) \\\\\n",
    "&, \\text{where } d_i = a_i = b_i \\forall i \\in \\{1, 2, \\dots, n - 2\\} \\\\\n",
    "(A \\times B)_{d_1, d_2, \\dots, d_{n - 2}, i, j} &=\n",
    "\\begin{cases}\n",
    "\\sum_{k = 1}^{b_{n - 1}} A_{i, k} \\times B_{k, j} & \\text{if } n = 2 \\\\\n",
    "\\sum_{k = 1}^{b_{n - 1}} A_{d_1, d_2, \\dots, d_{n - 2}, i, k} \\times B_{d_1, d_2, \\dots, d_{n - 2}, k, j} & \\text{if } n > 2\n",
    "\\end{cases} \\\\\n",
    "&, \\forall i \\in \\{1, \\dots, a_1\\}, j \\in \\{1, \\dots, b_2\\}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "例如：以 $A.\\text{size}() = (5, 4, 3)$ 與 $B.\\text{size}() = (5, 3, 2)$ 來說，$(A \\times B).\\text{size}() = (5, 4, 2)$。\n",
    "\n",
    "例如：以 $A.\\text{size}() = (1995, 10, 12, 5, 4, 3)$ 與 $B.\\text{size}() = (1995, 10, 12, 5, 3, 2)$ 來說，$(A \\times B).\\text{size}() = (1995, 10, 12, 5, 4, 2)$。\n",
    "\n",
    "在 `torch` 中張量乘法為 `torch.matmul(A, B)`。\n",
    "\n",
    "### 張量轉置（Tensor Transpose）\n",
    "\n",
    "令 $A$ 兩張量，$A.\\text{size}() = (a_1, a_2, ..., a_{n - 1}, a_n)$。定義 $A^{\\top}$ 如下：\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "A^{\\top} &= (A_{a_1, a_2, \\dots, a_{n - 2}, a_{n - 1}, a_n})^{\\top} \\\\\n",
    "&= A_{a_1, a_2, \\dots, a_{n - 2}, a_n, a_{n - 1}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "即交換張量 $A$ 的最後兩個維度。若想要指定不同的維度 $i, j$ 進行轉置，則定義 $A^{\\top_{i, j}}$ 如下：\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "A^{\\top_{i, j}} &= (A_{a_1, a_2, \\dots, a_i, \\dots, a_j, \\dots, a_n})^{\\top_i, j} \\\\\n",
    "&= A_{a_1, a_2, \\dots, a_j, \\dots, a_i, \\dots, a_n}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "例如：以 $A.\\text{size}() = (5, 4, 3)$ 來說，$A^{\\top_{1, 2}}.\\text{size}() = (5, 3, 4)$。\n",
    "\n",
    "例如：以 $A.\\text{size}() = (1995, 10, 12, 5, 4, 3)$ 來說，$A^{\\top_{3, 4}}.\\text{size}() = (1995, 10, 12, 4, 5, 3)$。\n",
    "\n",
    "在 `torch` 中張量轉置為 `torch.transpose(A, i, j)`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 張量乘法\n",
    "\n",
    "t25 = torch.ones(5, 4, 3)    # 宣告 Tensor 變數\n",
    "t26 = torch.ones(5, 3, 2)    # 宣告 Tensor 變數\n",
    "t27 = torch.matmul(t25, t26) # 進行張量乘法\n",
    "\n",
    "print(t25.size())            # 輸出張量 t25 的維度\n",
    "print(t26.size())            # 輸出張量 t26 的維度\n",
    "print(t27.size())            # 輸出張量 t27 的維度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 張量轉置\n",
    "\n",
    "t28 = torch.ones(5, 4, 3)                # 宣告 Tensor 變數\n",
    "\n",
    "print(torch.transpose(t28, 1, 2).size()) # 輸出轉置維度 1 與 2 後的維度\n",
    "print(t28.transpose(1, 2).size())        # 輸出轉置維度 1 與 2 後的維度\n",
    "\n",
    "print(torch.transpose(t28, 0, 2).size()) # 輸出轉置維度 0 與 2 後的維度\n",
    "print(t28.transpose(0, 2).size())        # 輸出轉置維度 0 與 2 後的維度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 維度運算\n",
    "\n",
    "### 降維函數（Dimension Decreasing Function）\n",
    "\n",
    "以下函數將會使**輸出**張量維度**小於輸入**張量維度。\n",
    "\n",
    "|函數|意義|\n",
    "|-|-|\n",
    "|`torch.sum`|將所有數值相加|\n",
    "|`torch.max`|取出所有數值中最大者|\n",
    "|`torch.min`|取出所有數值中最小者|\n",
    "|`torch.argmax`|取出所有數值中最大者的位置|\n",
    "|`torch.argmin`|取出所有數值中最小者的位置|\n",
    "|`torch.mean`|取出所有數值的平均值|\n",
    "|`torch.var`|取出所有數值的變異數|\n",
    "|`torch.std`|取出所有數值的標準差|\n",
    "|`torch.squeeze`|移除數字為 1 的維度|\n",
    "\n",
    "### 增維函數（Dimension Increasing Function）\n",
    "\n",
    "以下函數將會使**輸出**張量維度**大於輸入**張量維度。\n",
    "\n",
    "|函數|意義|\n",
    "|-|-|\n",
    "|`torch.cat`|串接多個相同維度的張量|\n",
    "|`torch.unsqueeze`|在指定的維度間增加 1 維度|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 降維函數\n",
    "\n",
    "t29 = torch.tensor([            # 宣告 Tensor 變數\n",
    "    [1, 2, 3, 4],\n",
    "    [5, 6, 7, 8],\n",
    "    [9, 10, 11, 12]\n",
    "])\n",
    "\n",
    "print(torch.sum(t29))           # 將張量 t29 中所有值相加\n",
    "print(t29.sum())                # 將張量 t29 中所有值相加\n",
    "\n",
    "print(torch.sum(t29, dim=0))    # 將張量 t29 中依照維度 0 將所有值相加\n",
    "print(t29.sum(dim=0))           # 將張量 t29 中依照維度 0 將所有值相加\n",
    "print(torch.sum(t29, dim=1))    # 將張量 t29 中依照維度 1 將所有值相加\n",
    "print(t29.sum(dim=1))           # 將張量 t29 中依照維度 1 將所有值相加\n",
    "\n",
    "print(torch.max(t29))           # 找出張量 t29 中最大值\n",
    "print(t29.max())                # 找出張量 t29 中最大值\n",
    "print()\n",
    "\n",
    "print(torch.max(t29, dim=0))    # 依照維度 0 找出張量 t29 中最大值\n",
    "print()                         # 並回傳最大值與對應位置\n",
    "print(t29.max(dim=0))           # 依照維度 0 找出張量 t29 中最大值\n",
    "print()                         # 並回傳最大值與對應位置\n",
    "print(torch.max(t29, dim=0)[0]) # 依照維度 0 找出張量 t29 中最大值\n",
    "print(torch.max(t29, dim=0)[1]) # 依照維度 0 找出張量 t29 中最大值位置\n",
    "print()\n",
    "\n",
    "print(torch.min(t29))           # 找出張量 t29 中最小值\n",
    "print(t29.min())                # 找出張量 t29 中最小值\n",
    "print()\n",
    "\n",
    "print(torch.min(t29, dim=1))    # 依照維度 1 找出張量 t29 中最小值\n",
    "print()                         # 並回傳最小值與對應位置\n",
    "print(t29.min(dim=1))           # 依照維度 1 找出張量 t29 中最小值\n",
    "print()                         # 並回傳最小值與對應位置\n",
    "print(torch.min(t29, dim=1)[0]) # 依照維度 1 找出張量 t29 中最小值\n",
    "print(torch.min(t29, dim=1)[1]) # 依照維度 1 找出張量 t29 中最小值位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t30 = torch.tensor([            # 宣告 Tensor 變數\n",
    "    [1, 2, 3, 4],\n",
    "    [5, 6, 7, 8],\n",
    "    [9, 10, 11, 12]\n",
    "])\n",
    "\n",
    "print(torch.argmax(t30))        # 找出張量 t30 中最大值的位置\n",
    "print(t30.argmax())             # 找出張量 t30 中最大值的位置\n",
    "\n",
    "print(torch.argmax(t30, dim=0)) # 依照維度 0 找出張量 t30 中最大值的位置\n",
    "print(t30.argmax(dim=0))        # 依照維度 0 找出張量 t30 中最大值的位置\n",
    "print(torch.argmax(t30, dim=1)) # 依照維度 1 找出張量 t30 中最大值的位置\n",
    "print(t30.argmax(dim=1))        # 依照維度 1 找出張量 t30 中最大值的位置\n",
    "\n",
    "print(torch.argmin(t30))        # 找出張量 t30 中最小值的位置\n",
    "print(t30.argmin())             # 找出張量 t30 中最小值的位置\n",
    "\n",
    "print(torch.argmin(t30, dim=0)) # 依照維度 0 找出張量 t30 中最小值的位置\n",
    "print(t30.argmin(dim=0))        # 依照維度 0 找出張量 t30 中最小值的位置\n",
    "print(torch.argmin(t30, dim=1)) # 依照維度 1 找出張量 t30 中最小值的位置\n",
    "print(t30.argmin(dim=1))        # 依照維度 1 找出張量 t30 中最小值的位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t31 = torch.tensor([            # 宣告 Tensor 變數\n",
    "    [1., 2., 3., 4.],\n",
    "    [5., 6., 7., 8.],\n",
    "    [9., 10., 11., 12.]\n",
    "])\n",
    "\n",
    "print(torch.mean(t31))         # 計算張量 t31 中所有值的平均數\n",
    "print(t31.mean())              # 計算張量 t31 中所有值的平均數\n",
    "\n",
    "print(torch.mean(t31, axis=0)) # 依照維度 0 計算張量 t31 中所有值的平均數\n",
    "print(t31.mean(axis=0))        # 依照維度 0 計算張量 t31 中所有值的平均數\n",
    "print(torch.mean(t31, axis=1)) # 依照維度 1 計算張量 t31 中所有值的平均數\n",
    "print(t31.mean(axis=1))        # 依照維度 1 計算張量 t31 中所有值的平均數\n",
    "\n",
    "print(torch.var(t31))          # 計算張量 t31 中所有值的變異數\n",
    "print(t31.var())               # 計算張量 t31 中所有值的變異數\n",
    "\n",
    "print(torch.var(t31, axis=0))  # 依照維度 0 計算張量 t31 中所有值的變異數\n",
    "print(t31.var(axis=0))         # 依照維度 0 計算張量 t31 中所有值的變異數\n",
    "print(torch.var(t31, axis=1))  # 依照維度 1 計算張量 t31 中所有值的變異數\n",
    "print(t31.var(axis=1))         # 依照維度 1 計算張量 t31 中所有值的變異數\n",
    "\n",
    "print(torch.std(t31))          # 計算張量 t31 中所有值的標準差\n",
    "print(t31.std())               # 計算張量 t31 中所有值的標準差\n",
    "\n",
    "print(torch.std(t31, axis=0))  # 依照維度 0 計算張量 t31 中所有值的標準差\n",
    "print(t31.std(axis=0))         # 依照維度 0 計算張量 t31 中所有值的標準差\n",
    "print(torch.std(t31, axis=1))  # 依照維度 1 計算張量 t31 中所有值的標準差\n",
    "print(t31.std(axis=1))         # 依照維度 1 計算張量 t31 中所有值的標準差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t32 = torch.tensor([        # 宣告 Tensor 變數\n",
    "    [1, 2, 3]\n",
    "])\n",
    "\n",
    "t32_sq = torch.squeeze(t32) # 移除張量 t32 中多餘的維度\n",
    "\n",
    "print(t32)                  # 輸出張量 t32\n",
    "print(t32.size())           # 輸出張量 t32 的維度\n",
    "print(t32_sq)               # 輸出移除維度後的張量 t32\n",
    "print(t32_sq.size())        # 輸出移除維度後張量 t32 的維度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 增維函數\n",
    "\n",
    "t33 = torch.tensor([                  # 宣告 Tensor 變數\n",
    "    [1, 2, 3]\n",
    "])\n",
    "\n",
    "t33_cat = torch.cat([                 # 串接多個張量 t33\n",
    "    t33,\n",
    "    t33,\n",
    "    t33,\n",
    "    t33\n",
    "]) \n",
    "\n",
    "print(t33_cat)                        # 輸出串接後的張量 t33_cat\n",
    "print(t33_cat.size())                 # 輸出串接後的張量 t33_cat 維度\n",
    "\n",
    "t34 = torch.tensor([                  # 宣告 Tensor 變數\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6]\n",
    "])\n",
    "\n",
    "print(t34)\n",
    "print(t34.size())\n",
    "\n",
    "t34_usq = torch.unsqueeze(t34, dim=0) # 對張量 t34 維度 0 增加 1 維\n",
    "\n",
    "print(t34_usq)                        # 輸出張量 t34 維度 0 增加 1 維後的結果\n",
    "print(t34_usq.size())                 # 輸出張量 t34 維度 0 增加 1 維後的維度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 GPU 運算\n",
    "\n",
    "上述的所有教學都是在 CPU 上進行運算，而大多數的深度學習框架都會提供操作 GPU 的介面幫助平型化運算。\n",
    "而 `torch` 與大部分的深度學習框架相同，使用 Nvidia 開發的 CUDA（Compute Unified Device Architecture）幫助使用 GPU 進行深度學習的運算（cuDNN）。\n",
    "\n",
    "使用 CUDA 操作平型化運算的流程為：\n",
    "\n",
    "1. 宣告 GPU 運算所需要佔用的記憶體（`cudaMalloc`）\n",
    "2. 定義每個平型化運算節點的運算內容\n",
    "3. 在主記憶體上創造資料（`malloc`）\n",
    "4. 將資料搬移至 GPU 的記憶體（`cudaMemcpy`）\n",
    "5. 每個節點獨立運算\n",
    "6. 將計算結果搬回至主記憶體（`memcpy`）\n",
    "7. 釋放 GPU 的記憶體（`cudaFree`）\n",
    "\n",
    "而在 `torch` 中將以上流程簡化成以下兩種方法\n",
    "\n",
    "- 宣告 `torch.Tensor` 變數時使用 `device='cuda:0'` 參數將變數宣告於 GPU 記憶體\n",
    "- 對已經創造於主記憶體的 `torch.Tensor` 變數使用 `torch.to('cuda:0')` 搬移至 GPU 記憶體\n",
    "\n",
    "```python\n",
    "torch.tensor([1., 2., 3.], device='cuda:0') # 使用 device 參數將變數宣告於 GPU 記憶體\n",
    "torch.tensor([1., 2., 3.]).to('cuda:0')     # 使用 to 將變數搬移至 GPU 記憶體\n",
    "```\n",
    "\n",
    "宣告於 GPU 或搬移至 GPU 後，之後所有的運算便會在 GPU 上進行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 GPU 運算\n",
    "\n",
    "try:\n",
    "    t35 = torch.tensor([1., 2., 3.], device='cuda:0') # 使用 device 參數創造張量於 GPU 上\n",
    "except:\n",
    "    print('torch not compiled with CUDA enabled')     # 如果不支援 cuda 則出現 error\n",
    "    t35 = None\n",
    "    \n",
    "print(t35)                                            # 輸出張量 t35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    t36 = torch.tensor([1., 2., 3.]).to('cuda:0') # 使用 to 將張量搬移至 GPU 上\n",
    "except:\n",
    "    print('torch not compiled with CUDA enabled') # 如果不支援 cuda 則出現 error\n",
    "    t36 = None\n",
    "    \n",
    "print(t36)                                        # 輸出張量 t36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():                   # 如果有可用 GPU 時採用 GPU cuda:0\n",
    "    device = torch.device('cuda:0')\n",
    "else:                                           # 若無 GPU 可用則使用 CPU\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "t37 = torch.tensor([1., 2., 3.], device=device) # 根據 device 創造張量\n",
    "t38 = torch.tensor([1., 2., 3.]).to(device)     # 使用 to 搬移張量至指定的裝置\n",
    "\n",
    "print(t37)                                      # 輸出張量 t37\n",
    "print(t38)                                      # 輸出張量 t38"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 深度學習\n",
    "\n",
    "![Deep Learning](https://miro.medium.com/max/1000/1*51D0MqtqHu3h2vTE5oJ-7g.png)\n",
    "\n",
    "使用 `torch` 進行深度學習主要包含以下步驟：\n",
    "\n",
    "1. 將資料轉換成 `torch.Tensor`\n",
    "2. 使用 `torch.nn` 建立深度學習模型架構\n",
    "3. 從 `torch.optim` 選擇最佳化工具\n",
    "4. 選擇目標函數\n",
    "5. 訓練深度學習模型\n",
    "6. 測試深度學習模型\n",
    "\n",
    "### 資料集\n",
    "\n",
    "使用 `torch.utils.data.Dataset` 將資料集轉換成 `torch.Tensor`：\n",
    "\n",
    "```python\n",
    "from torch.utils.data import Dataset             # 匯入資料集 base class\n",
    "\n",
    "class MyDataset(Dataset):                        # 繼承 base class 創造資料集\n",
    "    def __init__(self, data):                    # 給予資料\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)                    # 定義總資料數\n",
    "    \n",
    "    def __getitem__(self, index):                # 定義取出單一資料的方法\n",
    "        return torch.tensor(self.data[index])\n",
    "    \n",
    "my_dataset = MyDataset(np.array([1995, 10, 12])) # 創造資料集\n",
    "\n",
    "assert len(my_dataset) == 3                      # 取得總資料數\n",
    "assert my_dataset[0] == 1                        # 取出單一資料\n",
    "```\n",
    "\n",
    "使用 `torch.utils.data.DataLoader` 將資料集以匹次（mini-batch）取出：\n",
    "\n",
    "```python\n",
    "from torch.utils.data import DataLoader    # 匯入資料集抽樣工具\n",
    "\n",
    "my_data_loader = DataLoader(my_dataset,    # 對資料集 my_dataset 進行抽樣\n",
    "                            batch_size=10, # 設定每次抽樣的數量\n",
    "                            shuffle=True)  # 設定隨機抽樣\n",
    "\n",
    "for data in my_data_loader:                # 抽樣並進行某些計算\n",
    "    do_something\n",
    "```\n",
    "\n",
    "可以額外定義 `collate_fn` 將抽樣的資料整理成固定的格式：\n",
    "\n",
    "\n",
    "```python\n",
    "from torch.utils.data import DataLoader            # 匯入資料集抽樣工具\n",
    "\n",
    "def collate_fn(batch):                             # 定義格式化的方法\n",
    "    for data in batch:\n",
    "        do_something\n",
    "    \n",
    "    return formatted_data\n",
    "\n",
    "my_data_loader = DataLoader(my_dataset,            # 對資料集 my_dataset 進行抽樣\n",
    "                            batch_size=10,         # 設定每次抽樣的數量\n",
    "                            shuffle=True,          # 設定隨機抽樣\n",
    "                            collate_fn=collate_fn) # 指定格式化的方法\n",
    "```\n",
    "\n",
    "### 建立模型\n",
    "\n",
    "可以使用 `torch.nn` 現成的模型進行深度學習，常用的包含：\n",
    "\n",
    "|模型介面|名稱|常見用途|\n",
    "|-|-|-|\n",
    "|`torch.nn.Linear`|線性層（Linear Layer）|轉換特徵|\n",
    "|`torch.nn.Embedding`|嵌入層（Embedding Layer）|學習特徵向量表達法|\n",
    "|`torch.nn.Conv1d`|1 維卷積層（1-Dimensional Convolution Layer）|抽取連續資料區域特徵|\n",
    "|`torch.nn.Conv2d`|2 維卷積層（2-Dimensional Convolution Layer）|抽取平面圖片區域特徵|\n",
    "|`torch.nn.Conv3d`|3 維卷積層（3-Dimensional Convolution Layer）|抽取立體圖片區域特徵|\n",
    "|`torch.nn.RNN`|循環神經網路（Recurrent Neural Network）|壓縮動態長度文字|\n",
    "|`torch.nn.LSTM`|長短期記憶神經網路（Long Short-Term Memory）|有效壓縮動態長度文字|\n",
    "|`torch.nn.Transformer`|多面向自我注意力機制模型（Multi-Headed Self-Attention）|機器翻譯|\n",
    "\n",
    "```python\n",
    "import torch.nn as nn   # 匯入神經網路模型\n",
    "\n",
    "model = nn.Linear(      # 創造只包含線性層的模型\n",
    "    in_features=10,     # 設定線性層輸入維度\n",
    "    out_features=20     # 設定線性層輸出維度\n",
    ")\n",
    "\n",
    "pred_y = model(batch_x) # 輸入資料 batch_x 維度為 (batch_size, 10)\n",
    "                        # 輸入資料 pred_y 維度為 (batch_size, 20)\n",
    "```\n",
    "\n",
    "如果需要使用深度學習模型（即多個模型串接），則必須透過繼承 `torch.nn.Module` 來定義**模型結構**與**運算流程**：\n",
    "\n",
    "```python\n",
    "import torch.nn as nn                   # 匯入神經網路模型\n",
    "\n",
    "class MyModel(nn.Module):               # 模型需要繼承自 nn.Module\n",
    "    \n",
    "    def __init__(self):                 # 定義模型結構\n",
    "        super(MyModel, self).__init__() # 繼承 nn.Module 所有屬性\n",
    "        \n",
    "        self.layer1 = nn.Linear(        # 創造線性層 self.layer1\n",
    "            in_features=10,             # 設定線性層輸入維度\n",
    "            out_features=20             # 設定線性層輸出維度\n",
    "        )\n",
    "        self.layer2 = nn.Linear(        # 創造線性層 self.layer2\n",
    "            in_features=20,             # 設定線性層輸入維度\n",
    "            out_features=5              # 設定線性層輸出維度\n",
    "        )\n",
    "        \n",
    "    def forward(self, batch_x):         # 定義運算流程\n",
    "        h = self.layer1(batch_x)        # 使用線性層 self.layer1 輸入 batch_x 計算得到 h\n",
    "        y = self.layer2(h)              # 使用線性層 self.layer2 輸入 h 計算得到 y\n",
    "        return y                        # 輸出 y\n",
    "    \n",
    "my_model = MyModel()                    # 創造 MyModel 模型實例\n",
    "pred_y = my_model(batch_x)              # 自動呼叫 forward 計算 batch_x 得到 pred_y\n",
    "```\n",
    "\n",
    "使用 `nn.Module` 定義模型時必須要記得以下規則：\n",
    "\n",
    "- 在類別方法 `__init__` 中定義模型結構\n",
    "    - 必須要執行 `super(MyModel, self).__init__()`\n",
    "- 必須透過定義類別方法 `forward` 才能定義計算流程\n",
    "\n",
    "### 激發函數（Activation Functions）\n",
    "\n",
    "若模型需要激發函數，可以使用 `torch.nn.functional` 中事先定義好的激發函數：\n",
    "\n",
    "```python\n",
    "import torch.nn as nn                   # 匯入神經網路模型\n",
    "import torch.nn.functional as F         # 匯入激發函數\n",
    "\n",
    "class MyModel(nn.Module):               # 模型需要繼承自 nn.Module\n",
    "    \n",
    "    def __init__(self):                 # 定義模型結構\n",
    "        super(MyModel, self).__init__() # 繼承 nn.Module 所有屬性\n",
    "        \n",
    "        self.layer1 = nn.Linear(        # 創造線性層 self.layer1\n",
    "            in_features=10,             # 設定線性層輸入維度\n",
    "            out_features=20             # 設定線性層輸出維度\n",
    "        )\n",
    "        self.layer2 = nn.Linear(        # 創造線性層 self.layer2\n",
    "            in_features=20,             # 設定線性層輸入維度\n",
    "            out_features=5              # 設定線性層輸出維度\n",
    "        )\n",
    "        \n",
    "    def forward(self, batch_x):         # 定義運算流程\n",
    "        h = self.layer1(batch_x)        # 使用線性層 self.layer1 輸入 batch_x 計算得到 h\n",
    "        a = F.relu(h)                   # 使用 ReLU 激發函數輸入 h 得到 a\n",
    "        y = self.layer2(a)              # 使用線性層 self.layer2 輸入 a 計算得到 y\n",
    "        return F.softmax(y)             # 輸出 y 經過 softmax 後的結果\n",
    "    \n",
    "my_model = MyModel()                    # 創造 MyModel 模型實例\n",
    "pred_y = my_model(batch_x)              # 自動呼叫 forward 計算 batch_x 得到 pred_y\n",
    "```\n",
    "\n",
    "常見的激發函數包含：\n",
    "\n",
    "|激發函數|名稱|定義|數值範圍|\n",
    "|-|-|-|-|\n",
    "|`torch.nn.functional.relu`|ReLU|$$f(x_i) = \\max(0, x_i)$$|$$\\mathbb{R}^+$$|\n",
    "|`torch.nn.functional.softmax`|Softmax|$$f(x_i) = \\frac{e^{x_i}}{\\sum_{j = 0}^n e^{x_j}}$$|$$[0, 1]$$|\n",
    "|`torch.nn.functional.sigmoid`|Sigmoid|$$f(x_i) = \\frac{1}{1 + e^{-x_i}}$$|$$[0, 1]$$|\n",
    "|`torch.nn.functional.tanh`|Hyperbolic Tangent|$$f(x_i) = \\frac{e^{x_i} - e^{-x_i}}{e^{x_i} + e^{-x_i}}$$|$$[-1, 1]$$|\n",
    "\n",
    "### 目標函數（Objective Functions）\n",
    "\n",
    "使用 `torch.nn` 中事先定義好的目標函數進行模型最佳化，計算模型預測結果與標記訓練資料的誤差值，並透過向後傳播（Back Propagation）演算法取得相對於誤差值的梯度（Gradient）：\n",
    "\n",
    "```python\n",
    "import torch.nn as nn             # 匯入神經網路模型\n",
    "\n",
    "criterion = nn.MSELoss()          # 創造均方誤差計算工具\n",
    "\n",
    "pred_y = my_model(batch_x)        # 計算 batch_x 得到 pred_y\n",
    "loss = criterion(pred_y, batch_y) # 計算 pred_y 與 batch_y 的均方誤差\n",
    "\n",
    "loss.backward()                   # 使用向後傳播計算梯度\n",
    "```\n",
    "\n",
    "常見的目標函數包含：\n",
    "\n",
    "|目標函數|名稱|\n",
    "|-|-|\n",
    "|`torch.nn.MSELoss`|均方誤差（Mean Square Error）|\n",
    "|`torch.nn.CrossEntropyLoss`|交叉熵（Cross Entropy）|\n",
    "|`torch.nn.BCELoss`|二元交叉熵（Binary Cross Entropy）|\n",
    "|`torch.nn.NLLLoss`|負對數似然（Negative Log Likelihood）|\n",
    "\n",
    "### 最佳化（Optimization）\n",
    "\n",
    "![Gradient Descent](https://img-blog.csdnimg.cn/20181110102438617.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpX2tfeQ==,size_16,color_FFFFFF,t_70)\n",
    "\n",
    "使用 `torch.optim` 中的不同的最佳化策略進行梯度下降（Gradient Descent）演算法：\n",
    "\n",
    "$$\n",
    "\\theta_t = \\theta_{t - 1} - \\text{lr} \\cdot \\nabla \\mathcal{L}(x)\n",
    "$$\n",
    "\n",
    "\n",
    "```python\n",
    "import torch.nn as nn             # 匯入神經網路模型\n",
    "from torch.optim import SGD       # 匯入計算梯度下降演算法的工具\n",
    "\n",
    "criterion = nn.MSELoss()          # 創造均方誤差計算工具\n",
    "optimizer = SGD(                  # 創造計算隨機梯度下降的工具\n",
    "    my_model.parameters(),        # 設定計算梯度下降的目標\n",
    "    lr=0.01                       # 設定學習率\n",
    ")\n",
    "\n",
    "pred_y = my_model(batch_x)        # 計算 batch_x 得到 pred_y\n",
    "loss = criterion(pred_y, batch_y) # 計算 pred_y 與 batch_y 的均方誤差\n",
    "\n",
    "loss.backward()                   # 使用向後傳播計算梯度\n",
    "\n",
    "optimizer.step()                  # 使用梯度下降更新模型參數\n",
    "```\n",
    "\n",
    "進行最佳化時需要注意以下事項：\n",
    "\n",
    "- 創造最佳化工具時必須指定哪些參數被更新\n",
    "    - 使用 `model.parameters()` 取得模型中所有可以被更新的參數\n",
    "    - 學習率（Learning Rate）負責決定模型參數更新的幅度，可以透過 `lr` 參數設定\n",
    "- 必須先計算誤差並且透過誤差向後傳播（`loss.backward()`），才能執行梯度下降更新參數（`optimizer.step()`）\n",
    "\n",
    "### 測試\n",
    "\n",
    "深度學習模型在訓練時會自動計算梯度，若於分析模型在目標函數的表現時不想花多餘資源計算梯度可以使用 `with to.no_grad():`：\n",
    "\n",
    "```python\n",
    "with torch.no_grad():                 # 指定當前執行區塊不計算梯度\n",
    "    pred_y = my_model(batch_x)        # 計算 batch_x 得到 pred_y\n",
    "    loss = criterion(pred_y, batch_y) # 計算 pred_y 與 batch_y 的均方誤差\n",
    "    print(loss)                       # 輸出誤差\n",
    "```\n",
    "\n",
    "### 儲存 & 載入模型\n",
    "\n",
    "使用 `torch.save()` 配合 `model.state_dict()` 儲存訓練後的模型參數；\n",
    "使用 `model.load_state_dict()` 配合 `torch.load()` 載入儲存的訓練過的模型參數。\n",
    "\n",
    "```python\n",
    "torch.save(my_model.state_dict(), './data/model.ckpt')    # 儲存模型參數\n",
    "my_model.load_state_dict(torch.load('./data/model.ckpt')) # 載入模型參數\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 資料集\n",
    "\n",
    "from torch.utils.data import Dataset # 匯入資料集 base class\n",
    "\n",
    "class MyDataset(Dataset):            # 繼承 base class 創造資料集\n",
    "    def __init__(self, size):        # 給予資料集大小，並隨機創造資料\n",
    "        self.x = torch.rand(size) * 2 - 1\n",
    "        self.y = 2 * self.x ** 2 + 3 * self.x + 17\n",
    "        \n",
    "    def __len__(self):               # 定義總資料數\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, index):    # 定義取出單一資料的方法\n",
    "        return self.x[index], self.y[index]\n",
    "    \n",
    "my_dataset = MyDataset(10)           # 創造資料集\n",
    "\n",
    "print(len(my_dataset))               # 取得總資料數\n",
    "print(my_dataset[0])                 # 取出單一資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader # 匯入資料集抽樣工具\n",
    "\n",
    "def collate_fn(batch):                  # 定義格式化的方法\n",
    "    x_list = []\n",
    "    y_list = []\n",
    "    \n",
    "    for x, y in batch:\n",
    "        x_list.append([x])              # 將每個 x 轉換成 [x]\n",
    "        y_list.append([y])              # 將每個 y 轉換成 [y]\n",
    "        \n",
    "    return [torch.tensor(x_list),       # 最終回傳的維度為 (batch_size, features)\n",
    "            torch.tensor(y_list)]       # 最終回傳的維度為 (batch_size, labels)\n",
    "\n",
    "my_data_loader = DataLoader(            # 創造 DataLoader 實例\n",
    "    my_dataset,                         # 對資料集 my_dataset 進行抽樣\n",
    "    batch_size=3,                       # 設定每次抽樣的數量\n",
    "    shuffle=True,                       # 設定隨機抽樣\n",
    "    collate_fn=collate_fn               # 指定格式化的方法\n",
    ")\n",
    "\n",
    "for data in my_data_loader:             # 透過 my_data_loader 對資料集 my_dataset 進行抽樣\n",
    "    print(data)                         # 輸出抽樣結果\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立模型\n",
    "\n",
    "import torch.nn as nn                   # 匯入神經網路模型\n",
    "import torch.nn.functional as F         # 匯入激發函數\n",
    "\n",
    "class MyModel(nn.Module):               # 模型需要繼承自 nn.Module\n",
    "    def __init__(self,                  # 定義模型結構\n",
    "                 in_dim,                # 定義輸入層維度\n",
    "                 hid_dim,               # 定義隱藏層維度\n",
    "                 out_dim):              # 定義輸出層維度\n",
    "\n",
    "        super(MyModel, self).__init__() # 繼承 nn.Module 所有屬性\n",
    "        \n",
    "        self.layer1 = nn.Linear(        # 創造線性層 self.layer1\n",
    "            in_features=in_dim,         # 設定線性層輸入維度\n",
    "            out_features=hid_dim        # 設定線性層輸出維度\n",
    "        )\n",
    "        self.layer2 = nn.Linear(        # 創造線性層 self.layer2\n",
    "            in_features=hid_dim,        # 設定線性層輸入維度\n",
    "            out_features=out_dim        # 設定線性層輸出維度\n",
    "        )\n",
    "        \n",
    "    def forward(self, batch_x):         # 定義運算流程\n",
    "        h = self.layer1(batch_x)        # 使用線性層 self.layer1 輸入 batch_x 計算得到 h\n",
    "        a = F.relu(h)                   # 使用 ReLU 激發函數輸入 h 得到 a\n",
    "        y = self.layer2(a)              # 使用線性層 self.layer2 輸入 a 計算得到 y\n",
    "        return y                        # 輸出 y\n",
    "    \n",
    "my_model = MyModel(                     # 創造 MyModel 模型實例\n",
    "    in_dim=1,                           # 設定輸入層維度\n",
    "    hid_dim=10,                         # 設定隱藏層維度\n",
    "    out_dim=1                           # 設定輸出層維度\n",
    ")\n",
    "\n",
    "for batch_x, batch_y in my_data_loader: # 透過 my_data_loader 對資料集 my_dataset 進行抽樣\n",
    "    print(batch_x)\n",
    "    print(batch_y)\n",
    "    pred_y = my_model(batch_x)          # 自動呼叫 forward 計算 batch_x 得到 pred_y\n",
    "    print(pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 目標函數\n",
    "\n",
    "criterion = nn.MSELoss()                # 創造均方誤差計算工具\n",
    "\n",
    "for batch_x, batch_y in my_data_loader: # 透過 my_data_loader 對資料集 my_dataset 進行抽樣\n",
    "    \n",
    "    pred_y = my_model(batch_x)          # 自動呼叫 forward 計算 batch_x 得到 pred_y\n",
    "    \n",
    "    loss = criterion(pred_y, batch_y)   # 計算 pred_y 與 batch_y 的均方誤差\n",
    "    print(loss)\n",
    "    \n",
    "    loss.backward()                     # 使用向後傳播計算梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最佳化\n",
    "\n",
    "from torch.optim import SGD             # 匯入計算梯度下降演算法的工具\n",
    "\n",
    "optimizer = SGD(                        # 創造計算隨機梯度下降的工具\n",
    "    my_model.parameters(),              # 設定計算梯度下降的目標\n",
    "    lr=0.0001                           # 設定學習率\n",
    ")\n",
    "\n",
    "for batch_x, batch_y in my_data_loader: # 透過 my_data_loader 對資料集 my_dataset 進行抽樣\n",
    "    \n",
    "    pred_y = my_model(batch_x)          # 自動呼叫 forward 計算 batch_x 得到 pred_y\n",
    "    \n",
    "    loss = criterion(pred_y, batch_y)   # 計算 pred_y 與 batch_y 的均方誤差\n",
    "    loss.backward()                     # 使用向後傳播計算梯度\n",
    "    \n",
    "    optimizer.step()                    # 使用梯度下降更新模型參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 驗證\n",
    "\n",
    "if torch.cuda.is_available():                   # 如果有可用 GPU 時採用 GPU cuda:0\n",
    "    device = torch.device('cuda:0')\n",
    "else:                                           # 若無 GPU 可用則使用 CPU\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "train_dataset = MyDataset(1000)         # 創造訓練資料集\n",
    "test_dataset = MyDataset(500)           # 創造測試資料集\n",
    "\n",
    "# 設定超參數\n",
    "\n",
    "batch_size = 50                         # 設定每次抽樣的數量\n",
    "n_epoch = 5                             # 設定資料集總訓練次數\n",
    "hid_dim = 10                            # 設定隱藏層維度\n",
    "\n",
    "train_data_loader = DataLoader(         # 創造 DataLoader 實例\n",
    "    train_dataset,                      # 對資料集 train_dataset 進行抽樣\n",
    "    batch_size=batch_size,              # 設定每次抽樣的數量\n",
    "    shuffle=True,                       # 設定隨機抽樣\n",
    "    collate_fn=collate_fn               # 指定格式化的方法\n",
    ")\n",
    "test_data_loader = DataLoader(          # 創造 DataLoader 實例\n",
    "    test_dataset,                       # 對資料集 test_dataset 進行抽樣\n",
    "    batch_size=batch_size,              # 設定每次抽樣的數量\n",
    "    shuffle=True,                       # 設定隨機抽樣\n",
    "    collate_fn=collate_fn               # 指定格式化的方法\n",
    ")\n",
    "\n",
    "model = MyModel(                        # 創造 MyModel 模型實例\n",
    "    in_dim=1,                           # 設定輸入層維度\n",
    "    hid_dim=hid_dim,                    # 設定隱藏層維度\n",
    "    out_dim=1                           # 設定輸出層維度\n",
    ")\n",
    "model = model.to(device)                # 將模型搬移至 GPU\n",
    "\n",
    "criterion = nn.MSELoss()                # 創造均方誤差計算工具\n",
    "\n",
    "optimizer = SGD(                        # 創造計算隨機梯度下降的工具\n",
    "    model.parameters(),                 # 設定計算梯度下降的目標\n",
    "    lr=0.0001                           # 設定學習率\n",
    ")\n",
    "\n",
    "for epoch in range(n_epoch):            # 總共訓練 n_epoch 次\n",
    "    for batch_x, batch_y in train_data_loader:\n",
    "        batch_x = batch_x.to(device)    # 將訓練資料搬移至 GPU\n",
    "        batch_y = batch_y.to(device)    # 將訓練資料標記搬移至 GPU\n",
    "        \n",
    "        pred_y = model(batch_x)         # 自動呼叫 forward 計算 batch_x 得到 pred_y\n",
    "        loss = criterion(pred_y,        # 計算 pred_y 與 batch_y 的均方誤差\n",
    "                         batch_y)\n",
    "        \n",
    "        loss.backward()                 # 使用向後傳播計算梯度\n",
    "        optimizer.step()                # 使用梯度下降更新模型參數\n",
    "    \n",
    "    with torch.no_grad():               # 此區塊不會計算梯度\n",
    "        total_loss = 0                  # 統計訓練資料誤差\n",
    "        for batch_x, batch_y in train_data_loader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            \n",
    "            pred_y = model(batch_x)\n",
    "            loss = criterion(pred_y, batch_y)\n",
    "            \n",
    "            total_loss += float(loss) / len(train_data_loader)\n",
    "        \n",
    "        print('Epoch {}, training loss: {}'.format(epoch, total_loss))\n",
    "        \n",
    "        total_loss = 0                  # 統計測試資料誤差\n",
    "        for batch_x, batch_y in test_data_loader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            \n",
    "            pred_y = model(batch_x)\n",
    "            loss = criterion(pred_y, batch_y)\n",
    "            \n",
    "            total_loss += float(loss) / len(test_data_loader)\n",
    "            \n",
    "        print('Epoch {}, testing loss: {}'.format(epoch, total_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for batch_x, batch_y in train_data_loader:\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "\n",
    "        pred_y = model(batch_x)\n",
    "        \n",
    "        batch_x = batch_x.to('cpu')\n",
    "        batch_y = batch_y.to('cpu')\n",
    "        pred_y = pred_y.to('cpu')\n",
    "        plt.scatter(batch_x, batch_y, color='red') # 畫出訓練資料答案分佈\n",
    "        plt.scatter(batch_x, pred_y, color='blue') # 畫出訓練資料預測分佈\n",
    "        \n",
    "    plt.title('Training data performance')\n",
    "    plt.show()\n",
    "    \n",
    "    for batch_x, batch_y in test_data_loader:\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "\n",
    "        pred_y = model(batch_x)\n",
    "        \n",
    "        batch_x = batch_x.to('cpu')\n",
    "        batch_y = batch_y.to('cpu')\n",
    "        pred_y = pred_y.to('cpu')\n",
    "        plt.scatter(batch_x, batch_y, color='red') # 畫出測試資料答案分佈\n",
    "        plt.scatter(batch_x, pred_y, color='blue') # 畫出測試資料預測分佈\n",
    "    \n",
    "    plt.title('Testing data performance')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 儲存 & 載入模型\n",
    "\n",
    "torch.save(model.state_dict(), './data/model.ckpt')    # 儲存模型參數\n",
    "model.load_state_dict(torch.load('./data/model.ckpt')) # 載入模型參數"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 練習\n",
    "\n",
    "### 練習 1：調整超參數\n",
    "\n",
    "請試著更改前述範例中的超參數讓模型表現變好：\n",
    "\n",
    "- 增加訓練次數 `n_epoch`\n",
    "- 增大單一訓練資料次數 `batch_size`\n",
    "- 增大隱藏層的維度 `hid_dim`\n",
    "- 更改激發函數 `F.relu`\n",
    "\n",
    "### 練習 2：加深模型\n",
    "\n",
    "請試著更改前述範例中的模型深度讓模型表現變好：\n",
    "\n",
    "- 增加 1 個或多個 `nn.Linear`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
